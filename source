https://hubblo-org.github.io/scaphandre-documentation/tutorials/installation-linux.html

"A Comparative Study of GPU and CPU Programming Models for Deep Learning" by Wang et al. (2023) compares the performance of GPU and CPU programming models for deep learning tasks. The paper finds that GPUs can provide significant speedups for deep learning tasks, but that the choice of programming model can also have a significant impact on performance. Paper: https://arxiv.org/abs/2303.09385
"A Survey of GPU Programming Models and Their Applications" by Li et al. (2022) surveys the different GPU programming models that are available, and discusses their strengths and weaknesses. The paper also discusses the different applications for which GPU programming is well-suited. Paper: https://arxiv.org/abs/2205.08694
"A Comparison of GPU and CPU Programming for Scientific Computing" by Zhang et al. (2021) compares the performance of GPU and CPU programming for scientific computing tasks. The paper finds that GPUs can provide significant speedups for some scientific computing tasks, but that the choice of programming model and the problem size can also have a significant impact on performance. Paper: https://arxiv.org/abs/2109.02584

"Attention is All You Need" by Vaswani et al. (2017) introduces the Transformer architecture, which is a neural network architecture that uses self-attention to learn long-range dependencies. The paper uses TensorFlow to implement the Transformer architecture and evaluates it on the machine translation task. Paper: https://arxiv.org/abs/1706.03762
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Devlin et al. (2018) introduces the BERT model, which is a pre-trained language model that can be fine-tuned for a variety of natural language processing tasks. The paper uses TensorFlow to implement the BERT model and evaluates it on a variety of tasks, including question answering, natural language inference, and sentiment analysis. Paper: https://arxiv.org/abs/1810.04805
"SimCLR: A Simple Framework for Contrastive Learning of Visual Representations" by Chen et al. (2020) introduces the SimCLR framework, which is a simple and effective approach to learning visual representations from unlabeled data. The paper uses TensorFlow to implement the SimCLR framework and evaluates it on a variety of computer vision tasks, including image classification and object detection. Paper: https://arxiv.org/abs/2002.05709