{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgjZ1fvlORt6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9g6mrG6XK58",
        "outputId": "e296d15e-a200-422c-8ec0-987da706f364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading the mnist dataset\n",
        "# load dataset\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testY.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rihrd5YNQo-_",
        "outputId": "ea54527f-a65f-488f-91ea-5af8d513a047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape dataset to have a single channel\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "qL2sNFz7Qqkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ],
      "metadata": {
        "id": "NnpvWtxaQsVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "# normalize to range 0-1\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0"
      ],
      "metadata": {
        "id": "CcfGG8-WT8Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                            tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), padding='same', kernel_initializer='glorot_uniform', activation='selu', input_shape=(28, 28, 1)),\n",
        "                            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "                            tf.keras.layers.Flatten(),\n",
        "                            tf.keras.layers.Dense(64, kernel_initializer='glorot_uniform', activation='selu'),\n",
        "                            tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "1rW-MQckQw3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fio7iOyURMK2",
        "outputId": "91906ca1-4867-4834-d0ea-0691ff724e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                401472    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 402,442\n",
            "Trainable params: 402,442\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9qFDN2z7Rjr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU CODE"
      ],
      "metadata": {
        "id": "l8uOhzB3puh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = []\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "    epoch_info = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"epoch_memory_use\": 0,\n",
        "        \"epoch_time\": 0,\n",
        "        \"batch_processing_time\": 0,\n",
        "        \"throughput\": 0,\n",
        "        \"accuracy\": 0,\n",
        "        \"loss\": 0,\n",
        "    }\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step\n",
        "        with tf.device('/GPU:0'):\n",
        "            batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "        # Calculate memory usage (Note: This is an approximation)\n",
        "        memory_usage = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
        "        epoch_memory_use += memory_usage\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "    total_memory_usage += epoch_memory_use\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Memory Usage: {epoch_memory_use:.2f} bytes\")\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_info[\"epoch_memory_use\"] = epoch_memory_use\n",
        "    epoch_info[\"epoch_time\"] = epoch_time\n",
        "    epoch_info[\"batch_processing_time\"] = total_batch_processing_time\n",
        "    epoch_info[\"throughput\"] = throughput\n",
        "    epoch_info[\"accuracy\"] = accuracy\n",
        "    epoch_info[\"loss\"] = loss\n",
        "\n",
        "    epoch_data.append(epoch_info)\n",
        "\n",
        "\n",
        "with open('GPU_Python_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Total Memory Usage: {total_memory_usage:.2f} bytes\")\n",
        "print(f\"Average Memory Usage: {total_memory_usage / num_epochs:.2f} bytes\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSTZE1ePWRt1",
        "outputId": "876e9f2b-4fb7-460f-ce26-bf19a5e417e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " - Memory Usage: 212728879360.00 bytes\n",
            " - Epoch Time: 10.30 seconds\n",
            " - Batch Processing Time: 10.28 seconds\n",
            " - Throughput: 5824.50 samples/second\n",
            " - Accuracy: 0.9615\n",
            " - Loss: 0.1279\n",
            "Epoch 2/20\n",
            " - Memory Usage: 242522162176.00 bytes\n",
            " - Epoch Time: 8.65 seconds\n",
            " - Batch Processing Time: 8.63 seconds\n",
            " - Throughput: 6937.39 samples/second\n",
            " - Accuracy: 0.9618\n",
            " - Loss: 0.1152\n",
            "Epoch 3/20\n",
            " - Memory Usage: 274370733056.00 bytes\n",
            " - Epoch Time: 9.07 seconds\n",
            " - Batch Processing Time: 9.05 seconds\n",
            " - Throughput: 6614.66 samples/second\n",
            " - Accuracy: 0.9758\n",
            " - Loss: 0.0745\n",
            "Epoch 4/20\n",
            " - Memory Usage: 304162140160.00 bytes\n",
            " - Epoch Time: 9.13 seconds\n",
            " - Batch Processing Time: 9.11 seconds\n",
            " - Throughput: 6570.94 samples/second\n",
            " - Accuracy: 0.9781\n",
            " - Loss: 0.0700\n",
            "Epoch 5/20\n",
            " - Memory Usage: 333952733184.00 bytes\n",
            " - Epoch Time: 8.93 seconds\n",
            " - Batch Processing Time: 8.91 seconds\n",
            " - Throughput: 6718.67 samples/second\n",
            " - Accuracy: 0.9806\n",
            " - Loss: 0.0689\n",
            "Epoch 6/20\n",
            " - Memory Usage: 380211158528.00 bytes\n",
            " - Epoch Time: 8.39 seconds\n",
            " - Batch Processing Time: 8.37 seconds\n",
            " - Throughput: 7148.12 samples/second\n",
            " - Accuracy: 0.9792\n",
            " - Loss: 0.0782\n",
            "Epoch 7/20\n",
            " - Memory Usage: 242521355776.00 bytes\n",
            " - Epoch Time: 8.80 seconds\n",
            " - Batch Processing Time: 8.78 seconds\n",
            " - Throughput: 6817.36 samples/second\n",
            " - Accuracy: 0.9768\n",
            " - Loss: 0.0977\n",
            "Epoch 8/20\n",
            " - Memory Usage: 272312355840.00 bytes\n",
            " - Epoch Time: 9.01 seconds\n",
            " - Batch Processing Time: 8.99 seconds\n",
            " - Throughput: 6659.75 samples/second\n",
            " - Accuracy: 0.9797\n",
            " - Loss: 0.0839\n",
            "Epoch 9/20\n",
            " - Memory Usage: 304161733120.00 bytes\n",
            " - Epoch Time: 9.76 seconds\n",
            " - Batch Processing Time: 9.74 seconds\n",
            " - Throughput: 6146.73 samples/second\n",
            " - Accuracy: 0.9774\n",
            " - Loss: 0.0938\n",
            "Epoch 10/20\n",
            " - Memory Usage: 333952733184.00 bytes\n",
            " - Epoch Time: 8.80 seconds\n",
            " - Batch Processing Time: 8.78 seconds\n",
            " - Throughput: 6816.83 samples/second\n",
            " - Accuracy: 0.9790\n",
            " - Loss: 0.0929\n",
            "Epoch 11/20\n",
            " - Memory Usage: 363743733248.00 bytes\n",
            " - Epoch Time: 8.18 seconds\n",
            " - Batch Processing Time: 8.16 seconds\n",
            " - Throughput: 7338.70 samples/second\n",
            " - Accuracy: 0.9820\n",
            " - Loss: 0.0876\n",
            "Epoch 12/20\n",
            " - Memory Usage: 401768649728.00 bytes\n",
            " - Epoch Time: 9.15 seconds\n",
            " - Batch Processing Time: 9.13 seconds\n",
            " - Throughput: 6554.23 samples/second\n",
            " - Accuracy: 0.9787\n",
            " - Loss: 0.1049\n",
            "Epoch 13/20\n",
            " - Memory Usage: 431559242240.00 bytes\n",
            " - Epoch Time: 9.06 seconds\n",
            " - Batch Processing Time: 9.04 seconds\n",
            " - Throughput: 6622.81 samples/second\n",
            " - Accuracy: 0.9803\n",
            " - Loss: 0.0951\n",
            "Epoch 14/20\n",
            " - Memory Usage: 461350242304.00 bytes\n",
            " - Epoch Time: 8.73 seconds\n",
            " - Batch Processing Time: 8.71 seconds\n",
            " - Throughput: 6876.60 samples/second\n",
            " - Accuracy: 0.9830\n",
            " - Loss: 0.0833\n",
            "Epoch 15/20\n",
            " - Memory Usage: 288779373568.00 bytes\n",
            " - Epoch Time: 8.43 seconds\n",
            " - Batch Processing Time: 8.41 seconds\n",
            " - Throughput: 7118.31 samples/second\n",
            " - Accuracy: 0.9802\n",
            " - Loss: 0.1072\n",
            "Epoch 16/20\n",
            " - Memory Usage: 318570776832.00 bytes\n",
            " - Epoch Time: 9.01 seconds\n",
            " - Batch Processing Time: 8.99 seconds\n",
            " - Throughput: 6657.78 samples/second\n",
            " - Accuracy: 0.9826\n",
            " - Loss: 0.0965\n",
            "Epoch 17/20\n",
            " - Memory Usage: 350419750912.00 bytes\n",
            " - Epoch Time: 9.10 seconds\n",
            " - Batch Processing Time: 9.08 seconds\n",
            " - Throughput: 6590.77 samples/second\n",
            " - Accuracy: 0.9803\n",
            " - Loss: 0.1075\n",
            "Epoch 18/20\n",
            " - Memory Usage: 380210750976.00 bytes\n",
            " - Epoch Time: 9.00 seconds\n",
            " - Batch Processing Time: 8.98 seconds\n",
            " - Throughput: 6665.40 samples/second\n",
            " - Accuracy: 0.9840\n",
            " - Loss: 0.0922\n",
            "Epoch 19/20\n",
            " - Memory Usage: 410001751040.00 bytes\n",
            " - Epoch Time: 8.17 seconds\n",
            " - Batch Processing Time: 8.15 seconds\n",
            " - Throughput: 7347.30 samples/second\n",
            " - Accuracy: 0.9830\n",
            " - Loss: 0.0953\n",
            "Epoch 20/20\n",
            " - Memory Usage: 439792751104.00 bytes\n",
            " - Epoch Time: 8.86 seconds\n",
            " - Batch Processing Time: 8.85 seconds\n",
            " - Throughput: 6768.30 samples/second\n",
            " - Accuracy: 0.9807\n",
            " - Loss: 0.1155\n",
            "Total Training Time: 178.54 seconds\n",
            "Total Memory Usage: 6747093006336.00 bytes\n",
            "Average Memory Usage: 337354650316.80 bytes\n",
            "Average Throughput: 6739.76 samples/second\n",
            "Average Accuracy: 0.98\n",
            "Average Loss: 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU PART"
      ],
      "metadata": {
        "id": "X7M5FFY1pxZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = []\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "    epoch_info = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"epoch_time\": 0,\n",
        "        \"batch_processing_time\": 0,\n",
        "        \"throughput\": 0,\n",
        "        \"accuracy\": 0,\n",
        "        \"loss\": 0,\n",
        "    }\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step on CPU (no need to specify device)\n",
        "        batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_info[\"epoch_time\"] = epoch_time\n",
        "    epoch_info[\"batch_processing_time\"] = total_batch_processing_time\n",
        "    epoch_info[\"throughput\"] = throughput\n",
        "    epoch_info[\"accuracy\"] = accuracy\n",
        "    epoch_info[\"loss\"] = loss\n",
        "\n",
        "    epoch_data.append(epoch_info)\n",
        "\n",
        "\n",
        "\n",
        "with open('CPU_Python_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzuzu-ync_e",
        "outputId": "9cb81bcd-b59a-48e8-acc3-963cdd0ea77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " - Epoch Time: 47.81 seconds\n",
            " - Batch Processing Time: 47.80 seconds\n",
            " - Throughput: 1255.09 samples/second\n",
            " - Accuracy: 0.9819\n",
            " - Loss: 0.1197\n",
            "Epoch 2/20\n",
            " - Epoch Time: 46.17 seconds\n",
            " - Batch Processing Time: 46.17 seconds\n",
            " - Throughput: 1299.55 samples/second\n",
            " - Accuracy: 0.9821\n",
            " - Loss: 0.1108\n",
            "Epoch 3/20\n",
            " - Epoch Time: 45.97 seconds\n",
            " - Batch Processing Time: 45.96 seconds\n",
            " - Throughput: 1305.26 samples/second\n",
            " - Accuracy: 0.9766\n",
            " - Loss: 0.1692\n",
            "Epoch 4/20\n",
            " - Epoch Time: 46.33 seconds\n",
            " - Batch Processing Time: 46.33 seconds\n",
            " - Throughput: 1294.99 samples/second\n",
            " - Accuracy: 0.9825\n",
            " - Loss: 0.1225\n",
            "Epoch 5/20\n",
            " - Epoch Time: 46.26 seconds\n",
            " - Batch Processing Time: 46.25 seconds\n",
            " - Throughput: 1297.08 samples/second\n",
            " - Accuracy: 0.9810\n",
            " - Loss: 0.1289\n",
            "Epoch 6/20\n",
            " - Epoch Time: 45.09 seconds\n",
            " - Batch Processing Time: 45.09 seconds\n",
            " - Throughput: 1330.58 samples/second\n",
            " - Accuracy: 0.9790\n",
            " - Loss: 0.1497\n",
            "Epoch 7/20\n",
            " - Epoch Time: 45.04 seconds\n",
            " - Batch Processing Time: 45.04 seconds\n",
            " - Throughput: 1332.13 samples/second\n",
            " - Accuracy: 0.9808\n",
            " - Loss: 0.1365\n",
            "Epoch 8/20\n",
            " - Epoch Time: 46.74 seconds\n",
            " - Batch Processing Time: 46.74 seconds\n",
            " - Throughput: 1283.66 samples/second\n",
            " - Accuracy: 0.9799\n",
            " - Loss: 0.1563\n",
            "Epoch 9/20\n",
            " - Epoch Time: 46.25 seconds\n",
            " - Batch Processing Time: 46.24 seconds\n",
            " - Throughput: 1297.35 samples/second\n",
            " - Accuracy: 0.9814\n",
            " - Loss: 0.1459\n",
            "Epoch 10/20\n",
            " - Epoch Time: 46.29 seconds\n",
            " - Batch Processing Time: 46.29 seconds\n",
            " - Throughput: 1296.17 samples/second\n",
            " - Accuracy: 0.9795\n",
            " - Loss: 0.1673\n",
            "Epoch 11/20\n",
            " - Epoch Time: 45.47 seconds\n",
            " - Batch Processing Time: 45.46 seconds\n",
            " - Throughput: 1319.65 samples/second\n",
            " - Accuracy: 0.9805\n",
            " - Loss: 0.1357\n",
            "Epoch 12/20\n",
            " - Epoch Time: 45.33 seconds\n",
            " - Batch Processing Time: 45.33 seconds\n",
            " - Throughput: 1323.54 samples/second\n",
            " - Accuracy: 0.9816\n",
            " - Loss: 0.1371\n",
            "Epoch 13/20\n",
            " - Epoch Time: 45.01 seconds\n",
            " - Batch Processing Time: 45.00 seconds\n",
            " - Throughput: 1333.17 samples/second\n",
            " - Accuracy: 0.9793\n",
            " - Loss: 0.1573\n",
            "Epoch 14/20\n",
            " - Epoch Time: 44.85 seconds\n",
            " - Batch Processing Time: 44.85 seconds\n",
            " - Throughput: 1337.69 samples/second\n",
            " - Accuracy: 0.9809\n",
            " - Loss: 0.1457\n",
            "Epoch 15/20\n",
            " - Epoch Time: 45.84 seconds\n",
            " - Batch Processing Time: 45.83 seconds\n",
            " - Throughput: 1308.97 samples/second\n",
            " - Accuracy: 0.9803\n",
            " - Loss: 0.1540\n",
            "Epoch 16/20\n",
            " - Epoch Time: 46.00 seconds\n",
            " - Batch Processing Time: 45.99 seconds\n",
            " - Throughput: 1304.41 samples/second\n",
            " - Accuracy: 0.9818\n",
            " - Loss: 0.1579\n",
            "Epoch 17/20\n",
            " - Epoch Time: 46.31 seconds\n",
            " - Batch Processing Time: 46.30 seconds\n",
            " - Throughput: 1295.70 samples/second\n",
            " - Accuracy: 0.9808\n",
            " - Loss: 0.1563\n",
            "Epoch 18/20\n",
            " - Epoch Time: 44.83 seconds\n",
            " - Batch Processing Time: 44.83 seconds\n",
            " - Throughput: 1338.33 samples/second\n",
            " - Accuracy: 0.9812\n",
            " - Loss: 0.1549\n",
            "Epoch 19/20\n",
            " - Epoch Time: 45.03 seconds\n",
            " - Batch Processing Time: 45.03 seconds\n",
            " - Throughput: 1332.41 samples/second\n",
            " - Accuracy: 0.9806\n",
            " - Loss: 0.1732\n",
            "Epoch 20/20\n",
            " - Epoch Time: 45.50 seconds\n",
            " - Batch Processing Time: 45.49 seconds\n",
            " - Throughput: 1318.82 samples/second\n",
            " - Accuracy: 0.9801\n",
            " - Loss: 0.1820\n",
            "Total Training Time: 916.11 seconds\n",
            "Average Throughput: 1310.23 samples/second\n",
            "Average Accuracy: 0.98\n",
            "Average Loss: 0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXlIk8FRtbC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}