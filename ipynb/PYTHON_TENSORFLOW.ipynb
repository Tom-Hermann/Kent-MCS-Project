{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AgjZ1fvlORt6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9g6mrG6XK58",
        "outputId": "279ee680-1fdb-449f-896e-bb9b6b697aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading the mnist dataset\n",
        "# load dataset\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testY.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rihrd5YNQo-_",
        "outputId": "b77b5f94-24a4-479f-bc46-59b5abc0aadc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape dataset to have a single channel\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "qL2sNFz7Qqkc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ],
      "metadata": {
        "id": "NnpvWtxaQsVf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "# normalize to range 0-1\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0"
      ],
      "metadata": {
        "id": "CcfGG8-WT8Tj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMALL"
      ],
      "metadata": {
        "id": "sQI3aQtLd7ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"SMALL\"\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZHQEakzd73k",
        "outputId": "13a5997a-6b50-4640-b0d5-dae3cc3a0e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MEDIUM"
      ],
      "metadata": {
        "id": "cpG4UGU5d5D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"MEDIUM\"\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='selu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='selu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='selu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1rW-MQckQw3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f619c8-da90-482d-e4f1-eba0a57bae37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                200768    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 220,234\n",
            "Trainable params: 220,234\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BIG"
      ],
      "metadata": {
        "id": "FiHSpUokd7OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "model_type = \"MEDIUM\"\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='selu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), padding='same', strides=(1, 1), activation='selu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(256, (3, 3), padding='same', strides=(1, 1), activation='selu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='selu'),\n",
        "    tf.keras.layers.Dense(256, activation='selu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oFPJr7xd8Ac",
        "outputId": "b3e90a8d-c29e-4a52-cf3a-26693a9ec5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 3, 3, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               1180160   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,683,722\n",
            "Trainable params: 1,683,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile and summary"
      ],
      "metadata": {
        "id": "R5Z4dWBpeF88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9qFDN2z7Rjr7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU CODE"
      ],
      "metadata": {
        "id": "l8uOhzB3puh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = {\n",
        "    \"epoch\": [],\n",
        "    \"epoch_memory_usage\": [],\n",
        "    \"epoch_time\": [],\n",
        "    \"batch_processing_time\": [],\n",
        "    \"throughput\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"loss\": [],\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step\n",
        "        with tf.device('/GPU:0'):\n",
        "            batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "        # Calculate memory usage (Note: This is an approximation)\n",
        "        memory_usage = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
        "        epoch_memory_use += memory_usage\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "    total_memory_usage += epoch_memory_use\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Memory Usage: {epoch_memory_use:.2f} bytes\")\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_data[\"epoch\"].append(epoch + 1)\n",
        "    epoch_data[\"epoch_memory_usage\"].append(epoch_memory_use)\n",
        "    epoch_data[\"epoch_time\"].append(epoch_time)\n",
        "    epoch_data[\"batch_processing_time\"].append(total_batch_processing_time)\n",
        "    epoch_data[\"throughput\"].append(throughput)\n",
        "    epoch_data[\"accuracy\"].append(accuracy)\n",
        "    epoch_data[\"loss\"].append(loss)\n",
        "\n",
        "\n",
        "\n",
        "with open(f'{model_type}_GPU_PYTHON_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Total Memory Usage: {total_memory_usage:.2f} bytes\")\n",
        "print(f\"Average Memory Usage: {total_memory_usage / num_epochs:.2f} bytes\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSTZE1ePWRt1",
        "outputId": "a9652f2b-231b-4658-a369-97ff43b24ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " - Memory Usage: 91700360960.00 bytes\n",
            " - Epoch Time: 16.70 seconds\n",
            " - Batch Processing Time: 16.68 seconds\n",
            " - Throughput: 3592.85 samples/second\n",
            " - Accuracy: 0.9699\n",
            " - Loss: 0.0988\n",
            "Epoch 2/10\n",
            " - Memory Usage: 123790825472.00 bytes\n",
            " - Epoch Time: 12.77 seconds\n",
            " - Batch Processing Time: 12.75 seconds\n",
            " - Throughput: 4699.40 samples/second\n",
            " - Accuracy: 0.9789\n",
            " - Loss: 0.0833\n",
            "Epoch 3/10\n",
            " - Memory Usage: 162771043840.00 bytes\n",
            " - Epoch Time: 12.95 seconds\n",
            " - Batch Processing Time: 12.93 seconds\n",
            " - Throughput: 4634.10 samples/second\n",
            " - Accuracy: 0.9694\n",
            " - Loss: 0.1393\n",
            "Epoch 4/10\n",
            " - Memory Usage: 192562043392.00 bytes\n",
            " - Epoch Time: 12.88 seconds\n",
            " - Batch Processing Time: 12.86 seconds\n",
            " - Throughput: 4657.43 samples/second\n",
            " - Accuracy: 0.9842\n",
            " - Loss: 0.0715\n",
            "Epoch 5/10\n",
            " - Memory Usage: 222352764928.00 bytes\n",
            " - Epoch Time: 12.65 seconds\n",
            " - Batch Processing Time: 12.63 seconds\n",
            " - Throughput: 4744.28 samples/second\n",
            " - Accuracy: 0.9877\n",
            " - Loss: 0.0663\n",
            "Epoch 6/10\n",
            " - Memory Usage: 252143764992.00 bytes\n",
            " - Epoch Time: 12.75 seconds\n",
            " - Batch Processing Time: 12.73 seconds\n",
            " - Throughput: 4704.98 samples/second\n",
            " - Accuracy: 0.9859\n",
            " - Loss: 0.0665\n",
            "Epoch 7/10\n",
            " - Memory Usage: 121492761600.00 bytes\n",
            " - Epoch Time: 12.77 seconds\n",
            " - Batch Processing Time: 12.75 seconds\n",
            " - Throughput: 4700.11 samples/second\n",
            " - Accuracy: 0.9793\n",
            " - Loss: 0.0876\n",
            "Epoch 8/10\n",
            " - Memory Usage: 153581546496.00 bytes\n",
            " - Epoch Time: 13.08 seconds\n",
            " - Batch Processing Time: 13.06 seconds\n",
            " - Throughput: 4588.00 samples/second\n",
            " - Accuracy: 0.9796\n",
            " - Loss: 0.1109\n",
            "Epoch 9/10\n",
            " - Memory Usage: 192561764864.00 bytes\n",
            " - Epoch Time: 12.95 seconds\n",
            " - Batch Processing Time: 12.93 seconds\n",
            " - Throughput: 4632.38 samples/second\n",
            " - Accuracy: 0.9885\n",
            " - Loss: 0.0731\n",
            "Epoch 10/10\n",
            " - Memory Usage: 222352764928.00 bytes\n",
            " - Epoch Time: 12.90 seconds\n",
            " - Batch Processing Time: 12.88 seconds\n",
            " - Throughput: 4650.61 samples/second\n",
            " - Accuracy: 0.9905\n",
            " - Loss: 0.0587\n",
            "Total Training Time: 132.39 seconds\n",
            "Total Memory Usage: 1735309641472.00 bytes\n",
            "Average Memory Usage: 173530964147.20 bytes\n",
            "Average Throughput: 4560.41 samples/second\n",
            "Average Accuracy: 0.98\n",
            "Average Loss: 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU PART"
      ],
      "metadata": {
        "id": "X7M5FFY1pxZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = {\n",
        "    \"epoch\": [],\n",
        "    \"epoch_time\": [],\n",
        "    \"batch_processing_time\": [],\n",
        "    \"throughput\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"loss\": [],\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step on CPU (no need to specify device)\n",
        "        batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_data[\"epoch\"].append(epoch + 1)\n",
        "    epoch_data[\"epoch_time\"].append(epoch_time)\n",
        "    epoch_data[\"batch_processing_time\"].append(total_batch_processing_time)\n",
        "    epoch_data[\"throughput\"].append(throughput)\n",
        "    epoch_data[\"accuracy\"].append(accuracy)\n",
        "    epoch_data[\"loss\"].append(loss)\n",
        "\n",
        "\n",
        "\n",
        "with open('CPU_PYTHON_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzuzu-ync_e",
        "outputId": "bb444b22-709b-451f-c9e0-913b6ecdd9db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " - Epoch Time: 52.77 seconds\n",
            " - Batch Processing Time: 52.77 seconds\n",
            " - Throughput: 1136.96 samples/second\n",
            " - Accuracy: 0.9858\n",
            " - Loss: 0.0876\n",
            "Epoch 2/10\n",
            " - Epoch Time: 51.25 seconds\n",
            " - Batch Processing Time: 51.25 seconds\n",
            " - Throughput: 1170.72 samples/second\n",
            " - Accuracy: 0.9868\n",
            " - Loss: 0.0799\n",
            "Epoch 3/10\n",
            " - Epoch Time: 52.03 seconds\n",
            " - Batch Processing Time: 52.03 seconds\n",
            " - Throughput: 1153.21 samples/second\n",
            " - Accuracy: 0.9869\n",
            " - Loss: 0.0755\n",
            "Epoch 4/10\n",
            " - Epoch Time: 51.24 seconds\n",
            " - Batch Processing Time: 51.23 seconds\n",
            " - Throughput: 1171.06 samples/second\n",
            " - Accuracy: 0.9869\n",
            " - Loss: 0.0744\n",
            "Epoch 5/10\n",
            " - Epoch Time: 51.87 seconds\n",
            " - Batch Processing Time: 51.87 seconds\n",
            " - Throughput: 1156.65 samples/second\n",
            " - Accuracy: 0.9885\n",
            " - Loss: 0.0775\n",
            "Epoch 6/10\n",
            " - Epoch Time: 51.42 seconds\n",
            " - Batch Processing Time: 51.42 seconds\n",
            " - Throughput: 1166.79 samples/second\n",
            " - Accuracy: 0.9891\n",
            " - Loss: 0.0700\n",
            "Epoch 7/10\n",
            " - Epoch Time: 51.31 seconds\n",
            " - Batch Processing Time: 51.31 seconds\n",
            " - Throughput: 1169.28 samples/second\n",
            " - Accuracy: 0.9879\n",
            " - Loss: 0.0797\n",
            "Epoch 8/10\n",
            " - Epoch Time: 51.80 seconds\n",
            " - Batch Processing Time: 51.80 seconds\n",
            " - Throughput: 1158.20 samples/second\n",
            " - Accuracy: 0.9889\n",
            " - Loss: 0.0807\n",
            "Epoch 9/10\n",
            " - Epoch Time: 51.10 seconds\n",
            " - Batch Processing Time: 51.10 seconds\n",
            " - Throughput: 1174.13 samples/second\n",
            " - Accuracy: 0.9885\n",
            " - Loss: 0.0808\n",
            "Epoch 10/10\n",
            " - Epoch Time: 52.31 seconds\n",
            " - Batch Processing Time: 52.31 seconds\n",
            " - Throughput: 1146.97 samples/second\n",
            " - Accuracy: 0.9884\n",
            " - Loss: 0.0960\n",
            "Total Training Time: 517.12 seconds\n",
            "Average Throughput: 1160.40 samples/second\n",
            "Average Accuracy: 0.99\n",
            "Average Loss: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXlIk8FRtbC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}