{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AgjZ1fvlORt6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9g6mrG6XK58",
        "outputId": "6d306c42-455b-4c74-869c-35574488ace9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of loading the mnist dataset\n",
        "# load dataset\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testY.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rihrd5YNQo-_",
        "outputId": "d8a462d4-e32a-4ad3-f6e5-f9db35851d23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape dataset to have a single channel\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))"
      ],
      "metadata": {
        "id": "qL2sNFz7Qqkc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ],
      "metadata": {
        "id": "NnpvWtxaQsVf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "# normalize to range 0-1\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0"
      ],
      "metadata": {
        "id": "CcfGG8-WT8Tj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                            tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), padding='same', kernel_initializer='glorot_uniform', activation='selu', input_shape=(28, 28, 1)),\n",
        "                            tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "                            tf.keras.layers.Flatten(),\n",
        "                            tf.keras.layers.Dense(64, kernel_initializer='glorot_uniform', activation='selu'),\n",
        "                            tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "1rW-MQckQw3K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fio7iOyURMK2",
        "outputId": "a7b40d74-5c72-43d6-e144-ddf4c7231fb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                401472    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 402,442\n",
            "Trainable params: 402,442\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9qFDN2z7Rjr7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU CODE"
      ],
      "metadata": {
        "id": "l8uOhzB3puh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = {\n",
        "    \"epoch\": [],\n",
        "    \"epoch_memory_usage\": [],\n",
        "    \"epoch_time\": [],\n",
        "    \"batch_processing_time\": [],\n",
        "    \"throughput\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"loss\": [],\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step\n",
        "        with tf.device('/GPU:0'):\n",
        "            batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "        # Calculate memory usage (Note: This is an approximation)\n",
        "        memory_usage = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
        "        epoch_memory_use += memory_usage\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "    total_memory_usage += epoch_memory_use\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Memory Usage: {epoch_memory_use:.2f} bytes\")\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_data[\"epoch\"].append(epoch + 1)\n",
        "    epoch_data[\"epoch_memory_usage\"].append(epoch_memory_use)\n",
        "    epoch_data[\"epoch_time\"].append(epoch_time)\n",
        "    epoch_data[\"batch_processing_time\"].append(total_batch_processing_time)\n",
        "    epoch_data[\"throughput\"].append(throughput)\n",
        "    epoch_data[\"accuracy\"].append(accuracy)\n",
        "    epoch_data[\"loss\"].append(loss)\n",
        "\n",
        "\n",
        "\n",
        "with open('GPU_PYTHON_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Total Memory Usage: {total_memory_usage:.2f} bytes\")\n",
        "print(f\"Average Memory Usage: {total_memory_usage / num_epochs:.2f} bytes\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSTZE1ePWRt1",
        "outputId": "baa76e69-4e3f-4d14-8a44-78ab4733a695"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " - Memory Usage: 14002824960.00 bytes\n",
            " - Epoch Time: 11.35 seconds\n",
            " - Batch Processing Time: 11.33 seconds\n",
            " - Throughput: 5284.41 samples/second\n",
            " - Accuracy: 0.9646\n",
            " - Loss: 0.1137\n",
            "Epoch 2/10\n",
            " - Memory Usage: 44347799552.00 bytes\n",
            " - Epoch Time: 9.80 seconds\n",
            " - Batch Processing Time: 9.77 seconds\n",
            " - Throughput: 6124.86 samples/second\n",
            " - Accuracy: 0.9728\n",
            " - Loss: 0.0811\n",
            "Epoch 3/10\n",
            " - Memory Usage: 22005989888.00 bytes\n",
            " - Epoch Time: 9.91 seconds\n",
            " - Batch Processing Time: 9.88 seconds\n",
            " - Throughput: 6056.04 samples/second\n",
            " - Accuracy: 0.9743\n",
            " - Loss: 0.0778\n",
            "Epoch 4/10\n",
            " - Memory Usage: 44347799552.00 bytes\n",
            " - Epoch Time: 9.20 seconds\n",
            " - Batch Processing Time: 9.17 seconds\n",
            " - Throughput: 6524.51 samples/second\n",
            " - Accuracy: 0.9773\n",
            " - Loss: 0.0729\n",
            "Epoch 5/10\n",
            " - Memory Usage: 76549648384.00 bytes\n",
            " - Epoch Time: 10.05 seconds\n",
            " - Batch Processing Time: 10.03 seconds\n",
            " - Throughput: 5970.24 samples/second\n",
            " - Accuracy: 0.9719\n",
            " - Loss: 0.1004\n",
            "Epoch 6/10\n",
            " - Memory Usage: 106340648448.00 bytes\n",
            " - Epoch Time: 9.54 seconds\n",
            " - Batch Processing Time: 9.51 seconds\n",
            " - Throughput: 6291.38 samples/second\n",
            " - Accuracy: 0.9766\n",
            " - Loss: 0.0834\n",
            "Epoch 7/10\n",
            " - Memory Usage: 139873082880.00 bytes\n",
            " - Epoch Time: 9.63 seconds\n",
            " - Batch Processing Time: 9.61 seconds\n",
            " - Throughput: 6229.14 samples/second\n",
            " - Accuracy: 0.9629\n",
            " - Loss: 0.1718\n",
            "Epoch 8/10\n",
            " - Memory Usage: 169664082944.00 bytes\n",
            " - Epoch Time: 9.91 seconds\n",
            " - Batch Processing Time: 9.88 seconds\n",
            " - Throughput: 6057.49 samples/second\n",
            " - Accuracy: 0.9796\n",
            " - Loss: 0.0861\n",
            "Epoch 9/10\n",
            " - Memory Usage: 199455490048.00 bytes\n",
            " - Epoch Time: 10.55 seconds\n",
            " - Batch Processing Time: 10.53 seconds\n",
            " - Throughput: 5688.13 samples/second\n",
            " - Accuracy: 0.9800\n",
            " - Loss: 0.0825\n",
            "Epoch 10/10\n",
            " - Memory Usage: 229246083072.00 bytes\n",
            " - Epoch Time: 9.66 seconds\n",
            " - Batch Processing Time: 9.64 seconds\n",
            " - Throughput: 6211.89 samples/second\n",
            " - Accuracy: 0.9818\n",
            " - Loss: 0.0809\n",
            "Total Training Time: 99.58 seconds\n",
            "Total Memory Usage: 1045833449728.00 bytes\n",
            "Average Memory Usage: 104583344972.80 bytes\n",
            "Average Throughput: 6043.81 samples/second\n",
            "Average Accuracy: 0.97\n",
            "Average Loss: 0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU PART"
      ],
      "metadata": {
        "id": "X7M5FFY1pxZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import json\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epoch_data = {\n",
        "    \"epoch\": [],\n",
        "    \"epoch_memory_usage\": [],\n",
        "    \"epoch_time\": [],\n",
        "    \"batch_processing_time\": [],\n",
        "    \"throughput\": [],\n",
        "    \"accuracy\": [],\n",
        "    \"loss\": [],\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize variables for monitoring\n",
        "total_training_time = 0\n",
        "total_memory_usage = 0\n",
        "\n",
        "throughputs = []\n",
        "accs = []\n",
        "losss = []\n",
        "\n",
        "# Start training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Initialize variables for epoch-level monitoring\n",
        "    total_batch_processing_time = 0\n",
        "    epoch_memory_use = 0\n",
        "\n",
        "\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch in range(0, len(trainX), batch_size):\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "        # Extract a batch of data\n",
        "        batch_x = trainX[batch:batch + batch_size]\n",
        "        batch_y = trainY[batch:batch + batch_size]\n",
        "\n",
        "        # Perform training step on CPU (no need to specify device)\n",
        "        batch_history = model.train_on_batch(batch_x, batch_y)\n",
        "\n",
        "        batch_processing_time = time.time() - batch_start_time\n",
        "        total_batch_processing_time += batch_processing_time\n",
        "\n",
        "\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_time = epoch_end_time - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "\n",
        "    num_samples = len(trainX)\n",
        "    steps_per_epoch = num_samples // batch_size\n",
        "    throughput = num_samples / epoch_time\n",
        "\n",
        "    throughputs.append(throughput)\n",
        "\n",
        "    print(f\" - Epoch Time: {epoch_time:.2f} seconds\")\n",
        "    print(f\" - Batch Processing Time: {total_batch_processing_time:.2f} seconds\")\n",
        "    print(f\" - Throughput: {throughput:.2f} samples/second\")\n",
        "\n",
        "    # Evaluate accuracy and convergence\n",
        "    eval_results = model.evaluate(testX, testY, verbose=0)\n",
        "    accuracy = eval_results[1]\n",
        "    loss = eval_results[0]\n",
        "\n",
        "    accs.append(accuracy)\n",
        "    losss.append(loss)\n",
        "\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Loss: {loss:.4f}\")\n",
        "\n",
        "    epoch_data[\"epoch\"].append(epoch + 1)\n",
        "    epoch_data[\"epoch_memory_usage\"].append(0)\n",
        "    epoch_data[\"epoch_time\"].append(epoch_time)\n",
        "    epoch_data[\"batch_processing_time\"].append(total_batch_processing_time)\n",
        "    epoch_data[\"throughput\"].append(throughput)\n",
        "    epoch_data[\"accuracy\"].append(accuracy)\n",
        "    epoch_data[\"loss\"].append(loss)\n",
        "\n",
        "\n",
        "\n",
        "with open('CPU_PYTHON_epoch_data.json', 'w') as json_file:\n",
        "    json.dump(epoch_data, json_file, indent=4)\n",
        "\n",
        "print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
        "print(f\"Average Throughput: {sum(throughputs) / num_epochs:.2f} samples/second\")\n",
        "print(f\"Average Accuracy: {sum(accs) / num_epochs:.2f}\")\n",
        "print(f\"Average Loss: {sum(losss) / num_epochs:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnzuzu-ync_e",
        "outputId": "7d917592-6df7-4dd8-d652-ead0909d4f65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " - Epoch Time: 42.31 seconds\n",
            " - Batch Processing Time: 42.30 seconds\n",
            " - Throughput: 1418.26 samples/second\n",
            " - Accuracy: 0.9777\n",
            " - Loss: 0.0750\n",
            "Epoch 2/10\n",
            " - Epoch Time: 42.70 seconds\n",
            " - Batch Processing Time: 42.69 seconds\n",
            " - Throughput: 1405.25 samples/second\n",
            " - Accuracy: 0.9743\n",
            " - Loss: 0.0878\n",
            "Epoch 3/10\n",
            " - Epoch Time: 42.24 seconds\n",
            " - Batch Processing Time: 42.23 seconds\n",
            " - Throughput: 1420.59 samples/second\n",
            " - Accuracy: 0.9617\n",
            " - Loss: 0.1416\n",
            "Epoch 4/10\n",
            " - Epoch Time: 41.89 seconds\n",
            " - Batch Processing Time: 41.88 seconds\n",
            " - Throughput: 1432.43 samples/second\n",
            " - Accuracy: 0.9784\n",
            " - Loss: 0.0835\n",
            "Epoch 5/10\n",
            " - Epoch Time: 41.04 seconds\n",
            " - Batch Processing Time: 41.03 seconds\n",
            " - Throughput: 1462.08 samples/second\n",
            " - Accuracy: 0.9767\n",
            " - Loss: 0.0913\n",
            "Epoch 6/10\n",
            " - Epoch Time: 42.87 seconds\n",
            " - Batch Processing Time: 42.87 seconds\n",
            " - Throughput: 1399.54 samples/second\n",
            " - Accuracy: 0.9811\n",
            " - Loss: 0.0769\n",
            "Epoch 7/10\n",
            " - Epoch Time: 41.05 seconds\n",
            " - Batch Processing Time: 41.05 seconds\n",
            " - Throughput: 1461.64 samples/second\n",
            " - Accuracy: 0.9813\n",
            " - Loss: 0.0749\n",
            "Epoch 8/10\n",
            " - Epoch Time: 40.97 seconds\n",
            " - Batch Processing Time: 40.96 seconds\n",
            " - Throughput: 1464.55 samples/second\n",
            " - Accuracy: 0.9799\n",
            " - Loss: 0.0877\n",
            "Epoch 9/10\n",
            " - Epoch Time: 41.13 seconds\n",
            " - Batch Processing Time: 41.12 seconds\n",
            " - Throughput: 1458.95 samples/second\n",
            " - Accuracy: 0.9818\n",
            " - Loss: 0.0855\n",
            "Epoch 10/10\n",
            " - Epoch Time: 42.20 seconds\n",
            " - Batch Processing Time: 42.19 seconds\n",
            " - Throughput: 1421.91 samples/second\n",
            " - Accuracy: 0.9812\n",
            " - Loss: 0.1012\n",
            "Total Training Time: 418.37 seconds\n",
            "Average Throughput: 1434.52 samples/second\n",
            "Average Accuracy: 0.98\n",
            "Average Loss: 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXlIk8FRtbC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}